{
  "name": "Griptape Nodes Advanced Media Library",
  "library_schema_version": "0.3.0",
  "settings": [
    {
      "description": "Configuration settings for the Advanced Media Library temporary file storage",
      "category": "advanced_media_library",
      "contents": {
        "temp_folder_name": "intermediates"
      }
    },
    {
      "description": "Configuration settings for cleaning up the Advanced Media Library temporary file storage",
      "category": "advanced_media_library",
      "contents": {
        "max_directory_size_gb": 5,
        "enable_directory_cleanup": false
      }
    },
    {
      "description": "Configuration settings for image preview intermediates in diffusion pipeline runtime. Enables real-time image previews during generation but will slow down inference time.",
      "category": "advanced_media_library",
      "contents": {
        "enable_image_preview_intermediates": false
      }
    }
  ],
  "metadata": {
    "author": "Griptape, Inc.",
    "description": "Advanced media generation and manipulation nodes for Griptape Nodes.",
    "library_version": "0.62.0",
    "engine_version": "0.63.0",
    "tags": [
      "Griptape",
      "AI"
    ],
    "dependencies": {
      "pip_dependencies": [
        "accelerate>=1.6.0",
        "beautifulsoup4>=4.13.4",
        "controlnet-aux>=0.0.9",
        "static-ffmpeg>=2.8",
        "diffusers @ https://github.com/huggingface/diffusers/archive/5ffb73d4aeac9eaef8366d7b21872d64009bd1c7.zip",
        "imageio[ffmpeg]>=2.37.2",
        "ninja>=1.13.0",
        "numpy>=2.2.4",
        "opencv-python>=4.11.0.86",
        "peft>=0.17.0",
        "pillow>=11.2.1",
        "sentencepiece>=0.2.0",
        "cmake==3.31.6",
        "spandrel>=0.4.1",
        "torch>=2.7.0",
        "torchvision>=0.22.0",
        "torchaudio>=2.7.0",
        "transformers>=4.51.2",
        "tqdm>=4.67.1",
        "ultralytics>=8.0.0",
        "optimum-quanto>=0.2.7",
        "protobuf>=6.31.0",
        "prodigyopt>=1.1.2; sys_platform == 'win32'",
        "bitsandbytes>=0.46.0; sys_platform == 'win32'",
        "ftfy>=6.3.1",
        "sam2>=1.1.0",
        "scipy>=1.10.0",
        "matplotlib>=3.10.3",
        "supervision>=0.27.0",
        "griptape[drivers-prompt-amazon-bedrock,drivers-prompt-anthropic,drivers-prompt-cohere,drivers-prompt-ollama,drivers-web-scraper-trafilatura,drivers-web-search-duckduckgo,drivers-web-search-exa,loaders-image]>=1.8.12"
      ],
      "pip_install_flags": [
        "--preview",
        "--torch-backend=auto"
      ]
    }
  },
  "categories": [],
  "nodes": [
    {
      "class_name": "DepthAnythingForDepthEstimationImage",
      "file_path": "transformers_nodes_library/depth_anything_for_depth_estimation_image.py",
      "metadata": {
        "category": "image/depth",
        "description": "Generate a Depth Map with Depth Anything V2 and ðŸ¤— Transformers Pairs well with ControlNet Image Generation in depth mode.",
        "display_name": "Depth Anything V2 Image"
      }
    },
    {
      "class_name": "YOLOv8FaceDetection",
      "file_path": "ultralytics_nodes_library/yolov8_face_detection.py",
      "metadata": {
        "category": "image/detection",
        "description": "Detect faces in images using a fine-tuned Ultralytics YOLOv8 model from Hugging Face. Returns bounding boxes with confidence scores for each detected face.",
        "display_name": "YOLOv8 Face Detection"
      }
    },
    {
      "class_name": "OpenPoseImageDetection",
      "file_path": "openpose_nodes_library/openpose_image_detection.py",
      "metadata": {
        "category": "image/pose",
        "description": "Detect human poses in images using OpenPose models converted to SafeTensors format",
        "display_name": "OpenPose Image Detection"
      }
    },
    {
      "class_name": "DinoSam2ImageDetector",
      "file_path": "dino_sam2_library/dino_sam_2_image_detector.py",
      "metadata": {
        "category": "image/segmentation",
        "description": "Generate an image mask from a prompt. Uses Grounding DINO to transform prompt to bounding boxes, then uses Segment Anything Model 2 (SAM2) from Meta to generate the mask from the bounding boxes.",
        "display_name": "Image Mask via G-DINO + SAM2"
      }
    },
    {
      "class_name": "DepthAnythingForDepthEstimationVideo",
      "file_path": "transformers_nodes_library/depth_anything_for_depth_estimation_video.py",
      "metadata": {
        "category": "video/depth",
        "description": "Generate Depth Maps for video with Depth Anything V2 and ðŸ¤— Transformers. Processes each frame for depth estimation.",
        "display_name": "Depth Anything V2 Video"
      }
    },
    {
      "class_name": "OpenPoseVideoDetection",
      "file_path": "openpose_nodes_library/openpose_video_detection.py",
      "metadata": {
        "category": "video/pose",
        "description": "Detect human poses in videos using OpenPose models converted to SafeTensors format",
        "display_name": "OpenPose Video Detection"
      }
    },
    {
      "class_name": "DinoSam2VideoDetector",
      "file_path": "dino_sam2_library/dino_sam_2_video_detector.py",
      "metadata": {
        "category": "video/segmentation",
        "description": "Generate a video mask from a prompt. Uses Grounding DINO to transform prompt to bounding boxes on a single frame, then uses Segment Anything Model 2 (SAM2) from Meta to generate the mask from the bounding boxes across all frames.",
        "display_name": "Video Mask via G-DINO + SAM2"
      }
    },
    {
      "class_name": "CannyConvertImage",
      "file_path": "opencv_nodes_library/canny_convert_image.py",
      "metadata": {
        "category": "image/edge",
        "description": "Detect Edges with OpenCV. Pairs well with ControlNet Image Generation in edge mode.",
        "display_name": "Detect Edges with OpenCV"
      }
    },
    {
      "class_name": "MorphologyImage",
      "file_path": "opencv_nodes_library/morphology_image.py",
      "metadata": {
        "category": "image/edge",
        "description": "Apply OpenCV morphological operations (erode, dilate, open, close) to grayscale images",
        "display_name": "Morphology",
        "icon": "circle-dot"
      }
    },
    {
      "class_name": "AnylineDetector",
      "file_path": "controlnet_aux_nodes_library/anyline_detector.py",
      "metadata": {
        "category": "image/edge",
        "description": "Detect Edges with Anyline. Pairs well with ControlNet Image Generation in edge mode.",
        "display_name": "Detect Edges with Anyline"
      }
    },
    {
      "class_name": "LumatalesFluxLora",
      "file_path": "diffusers_nodes_library/pipelines/flux/lora/lumatales_flux_lora.py",
      "metadata": {
        "category": "lora/flux",
        "description": "Load the Flux LoRA for use with ðŸ¤— Diffusers based Flux Nodes. ðŸ¤— Model Card: https://huggingface.co/Shakker-Labs/Lumatales-FL",
        "display_name": "Flux LoRA: Lumatales-FL ",
        "group": "diffusion"
      }
    },
    {
      "class_name": "MicroLandscapeOnPhoneFluxLora",
      "file_path": "diffusers_nodes_library/pipelines/flux/lora/micro_landscape_on_phone_flux_lora.py",
      "metadata": {
        "category": "lora/flux",
        "description": "Load the Flux LoRA for use with ðŸ¤— Diffusers based Flux Nodes. ðŸ¤— Model Card: https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Micro-landscape-on-Mobile-Phone",
        "display_name": "Flux LoRA: Micro Landscape on Mobile Phone",
        "group": "diffusion"
      }
    },
    {
      "class_name": "MiniatureWorldFluxLora",
      "file_path": "diffusers_nodes_library/pipelines/flux/lora/miniature_world_flux_lora.py",
      "metadata": {
        "category": "lora/flux",
        "description": "Load the Flux LoRA for use with ðŸ¤— Diffusers based Flux Nodes. ðŸ¤— Model Card: https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Miniature-World",
        "display_name": "Flux LoRA: Miniature World",
        "group": "diffusion"
      }
    },
    {
      "class_name": "RiverZNormalDiptychFluxFillLora",
      "file_path": "diffusers_nodes_library/pipelines/flux/lora/river_z_normal_diptych_flux_fill_lora.py",
      "metadata": {
        "category": "lora/flux",
        "description": "Load the Flux LoRA for use with the ICEdit Image with Flux Node. ðŸ¤— Model Card:https://huggingface.co/RiverZ/normal-lora",
        "display_name": "Flux ICEdit LoRA: RiverZ Normal",
        "group": "diffusion"
      }
    },
    {
      "class_name": "TrainFluxLora",
      "file_path": "diffusers_nodes_library/pipelines/flux/peft/train_flux_lora.py",
      "metadata": {
        "category": "image/flux/loras",
        "description": "TrainFluxLora node.",
        "display_name": "Train Flux LoRA"
      }
    },
    {
      "class_name": "FirstFrameToVideoWanVaceAux",
      "file_path": "diffusers_nodes_library/pipelines/wan/auxiliary/first_frame_to_video_wan_vace_aux.py",
      "metadata": {
        "category": "video/wan/aux",
        "description": "Generate video with input image as first frame for WAN VACE conditioning.",
        "display_name": "WAN First Frame Aux"
      }
    },
    {
      "class_name": "FirstLastFrameToVideoWanVaceAux",
      "file_path": "diffusers_nodes_library/pipelines/wan/auxiliary/first_last_frame_to_video_wan_vace_aux.py",
      "metadata": {
        "category": "video/wan/aux",
        "description": "Generate video with input images as first and last frames for WAN VACE conditioning.",
        "display_name": "WAN First+Last Frame Aux"
      }
    },
    {
      "class_name": "LastFrameToVideoWanVaceAux",
      "file_path": "diffusers_nodes_library/pipelines/wan/auxiliary/last_frame_to_video_wan_vace_aux.py",
      "metadata": {
        "category": "video/wan/aux",
        "description": "Generate video with input image as last frame for WAN VACE conditioning.",
        "display_name": "WAN Last Frame Aux"
      }
    },
    {
      "class_name": "RandomFramesToVideoWanVaceAux",
      "file_path": "diffusers_nodes_library/pipelines/wan/auxiliary/random_frames_to_video_wan_vace_aux.py",
      "metadata": {
        "category": "video/wan/aux",
        "description": "Generate video with input images at random frame positions for WAN VACE conditioning.",
        "display_name": "WAN Random Frames Aux"
      }
    },
    {
      "class_name": "StaticMaskWanVaceAux",
      "file_path": "diffusers_nodes_library/pipelines/wan/auxiliary/static_mask_wan_vace_aux.py",
      "metadata": {
        "category": "video/wan/aux",
        "description": "Generate static mask video for WAN VACE conditioning.",
        "display_name": "WAN Static Mask Aux"
      }
    },
    {
      "class_name": "Kijai1Dot3BWanLora",
      "file_path": "diffusers_nodes_library/pipelines/wan/lora/kijai_1_dot_3_b_wan_lora.py",
      "metadata": {
        "category": "lora/wan",
        "description": "Experimental LoRA extractions from CausVid finetunes. Enables generating videos in 2-8 steps.",
        "display_name": "Kijai CausVid 1.3B",
        "group": "diffusion"
      }
    },
    {
      "class_name": "Kijai14BWanLora",
      "file_path": "diffusers_nodes_library/pipelines/wan/lora/kijai_14_b_wan_lora.py",
      "metadata": {
        "category": "lora/wan",
        "description": "Experimental LoRA extractions from CausVid finetunes. Enables generating videos in 2-8 steps.",
        "display_name": "Kijai CausVid 14B",
        "group": "diffusion"
      }
    },
    {
      "class_name": "DiffusionPipelineBuilderNode",
      "file_path": "diffusers_nodes_library/common/nodes/diffusion_pipeline_builder_node.py",
      "metadata": {
        "category": "image",
        "description": "Build and cache ðŸ¤— Diffusers Pipelines for reuse across multiple execution nodes.",
        "display_name": "Diffusion Pipeline Builder",
        "group": "diffusion"
      }
    },
    {
      "class_name": "DiffusionPipelineRuntimeNode",
      "file_path": "diffusers_nodes_library/common/nodes/diffusion_pipeline_runtime_node.py",
      "metadata": {
        "category": "image",
        "description": "Generate images via ðŸ¤— Diffusers Pipelines.",
        "display_name": "Generate Image (Diffusion Pipeline)",
        "group": "diffusion"
      }
    },
    {
      "class_name": "LoadLora",
      "file_path": "diffusers_nodes_library/common/nodes/load_lora_node.py",
      "metadata": {
        "category": "lora",
        "description": "Load LoRA from file ('.safetensors', '.pt', '.bin', '.json', '.lora') for use with ðŸ¤— Diffusers based Nodes. Path must be on the engine's filesystem.",
        "display_name": "Load LoRA",
        "group": "diffusion"
      }
    }
  ],
  "workflows": [
    "workflows/templates/qwen_edit_2509_camera_control.py"
  ]
}
